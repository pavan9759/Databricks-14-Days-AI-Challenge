{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4900faf-4f9c-4a58-b375-cc3b73286bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"ecommerce.silver.events_part\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29d8086-93ee-4dd7-99c7-1203d6a591fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Descriptive Statistics (Statistical Summary)\n",
    "#Count, Average, Min, Max (Price)\n",
    "\n",
    "from pyspark.sql.functions import avg, min, max, count\n",
    "\n",
    "df.select(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "    max(\"price\").alias(\"max_price\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fb9ed8-5837-4ce6-afc1-b54d98e43fae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, when\n",
    "\n",
    "df2 = df.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when(dayofweek(\"event_date\").isin([1,7]), \"Weekend\")\n",
    "    .otherwise(\"Weekday\") \n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1055965a-6beb-4d6c-a86a-d4afe4dd2e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Compare average price (proxy for sales)  \n",
    "df2.groupBy(\"is_weekend\") \\\n",
    "   .agg(avg(\"price\").alias(\"avg_price\")) \\\n",
    "   .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc83d9d-7a99-4e74-a7c8-36926bc6fef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Correlation Analysis\n",
    "#Correlation between price and product_id (demo)\n",
    "\n",
    "df.selectExpr(\n",
    "    \"corr(price, product_id) as price_product_corr\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa83b907-30b7-402e-bad1-c1f858ee1013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Feature Engineering (ML Prep)\n",
    "#Create ML-ready features\n",
    "\n",
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "features_df = df2.withColumn(\"year\", year(\"event_date\")) \\\n",
    "                 .withColumn(\"month\", month(\"event_date\")) \\\n",
    "                 .withColumn(\n",
    "                     \"is_purchase\",\n",
    "                     when(df2.event_type == \"purchase\", 1).otherwise(0)\n",
    "                 )\n",
    "\n",
    "features_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc87dc6-8c6e-4a66-9d59-1bb180da362d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "\n",
    "user_features = features_df.groupBy(\"user_id\").agg(\n",
    "    count(\"*\").alias(\"total_events\"),\n",
    "    avg(\"price\").alias(\"avg_spend\"),\n",
    "    sum(\"is_purchase\").alias(\"total_purchases\")\n",
    ")\n",
    "\n",
    "user_features.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 11",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
